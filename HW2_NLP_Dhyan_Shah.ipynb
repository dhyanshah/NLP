{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSDS  7337-404 - Natural Language Processing ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework2 ###\n",
    "**Shah Dhyan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is python:\n",
      "3.7.4 (default, Aug 13 2019, 15:17:50) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "\n",
      "nltk 3.4.5\n",
      "bs4 4.8.0\n",
      "re 2.2.1\n",
      "requests 2.22.0\n"
     ]
    }
   ],
   "source": [
    "import sys;\n",
    "print('This is python:')\n",
    "print(sys.version)\n",
    "print('')\n",
    "\n",
    "import nltk; print( 'nltk ' + nltk.__version__)\n",
    "from nltk import word_tokenize;\n",
    "\n",
    "import bs4; print( 'bs4 ' + bs4.__version__)\n",
    "from bs4 import BeautifulSoup, SoupStrainer;\n",
    "\n",
    "import re; print('re ' + re.__version__)\n",
    "import requests; print('requests ' + requests.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1 ## \n",
    "**The simple approach considered is to divide each text length by the highest vocabulary size. This helps interpret results with a linear scaling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gutenberg.org/wiki/Technology_(Bookshelf)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_url(url):\n",
    "    return requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links_from_html(html):\n",
    "    tags = BeautifulSoup(html, 'html.parser', parse_only=SoupStrainer('a', href=True))\n",
    "    urls = [str(tag.attrs['href']) for tag in tags]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevent_link(link):\n",
    "    if '/ebooks/' in link:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_ids_from_links(links):\n",
    "    return [link.split('/ebooks/').pop() for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_text_urls_from_ids(ids):\n",
    "    url_template = 'http://www.gutenberg.org/files/{}/{}.txt'\n",
    "    # url_template = 'http://www.gutenberg.org/files/{}/{}-h/{}-h.htm'\n",
    "    return [url_template.format(id, id, id) for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 233 books\n",
      "fetching book via http://www.gutenberg.org/files/19533/19533.txt found text with 115097 characters\n",
      "fetching book via http://www.gutenberg.org/files/24681/24681.txt found text with 125855 characters\n",
      "fetching book via http://www.gutenberg.org/files/33912/33912.txt found text with 89433 characters\n",
      "fetching book via http://www.gutenberg.org/files/18448/18448.txt found text with 4430 characters\n",
      "fetching book via http://www.gutenberg.org/files/21281/21281.txt found text with 58979 characters\n",
      "fetching book via http://www.gutenberg.org/files/23319/23319.txt found text with 427253 characters\n",
      "fetching book via http://www.gutenberg.org/files/29269/29269.txt found text with 63846 characters\n",
      "fetching book via http://www.gutenberg.org/files/27632/27632.txt found text with 392837 characters\n",
      "fetching book via http://www.gutenberg.org/files/25269/25269.txt found text with 283990 characters\n",
      "fetching book via http://www.gutenberg.org/files/32282/32282.txt found text with 107219 characters\n"
     ]
    }
   ],
   "source": [
    "all_texts = []\n",
    "\n",
    "def fetch_all_texts():\n",
    "    if len(all_texts) > 0:\n",
    "        print(len(all_texts))\n",
    "        return\n",
    "    # fetch the html from the home page \n",
    "    home_page_html = get_text_from_url(url_to_fetch_images)\n",
    "    # collect links from the html\n",
    "    all_links_from_home_page = get_all_links_from_html(home_page_html)\n",
    "    # removing links that can't be used\n",
    "    relevent_links_from_home_page = list(filter(relevent_link, all_links_from_home_page))\n",
    "\n",
    "    book_ids = get_book_ids_from_links(relevent_links_from_home_page)\n",
    "    book_urls = get_book_text_urls_from_ids(book_ids)\n",
    "    print('found {} books'.format(len(book_urls)))\n",
    "    book_i = 0\n",
    "    for url in book_urls:\n",
    "        print ('fetching book via {}'.format(url), end=' ')\n",
    "        book_text = get_text_from_url(url)\n",
    "        print('found text with {} characters'.format(len(book_text)))\n",
    "        all_texts.append({\n",
    "            'url': url,\n",
    "            'corpus': book_text\n",
    "        })\n",
    "\n",
    "        book_i += 1\n",
    "        # maximum number of book fetch\n",
    "        if book_i == 10:\n",
    "            return\n",
    "        \n",
    "    \n",
    "    # adding text to dictionary \n",
    "    return 1\n",
    "fetch_all_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_corpus(corpus):\n",
    "    # useing regex to only consider alphabets \n",
    "    corpus = re.sub('[^a-zA-Z\\s]', '', corpus)\n",
    "    # select lowercase of all letter\n",
    "    corpus = corpus.lower()\n",
    "    # tokanizing words\n",
    "    return word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(words):\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'h', 's'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_unique_words('shah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocabulary_score(vocab_size, highest_vocab_size):\n",
    "    return float(vocab_size) / highest_vocab_size\n",
    "\n",
    "get_vocabulary_score(10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2 ## \n",
    "**To obtain the long word score in each book corpus, simply counted the long words per corpus and divided it by the highest of the score.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_long_word_count(vocab):\n",
    "    minimum_size = 10\n",
    "    long_words = [t for t in vocab if len(t) >= minimum_size]\n",
    "    return len(long_words)\n",
    "\n",
    "get_long_word_count(['tc', 'MeteorologicalInstruments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_diversity(words, unique_words):\n",
    "    return float(len(unique_words)) / len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3 ## \n",
    "**To calculated number of unique words and divided by total # of words for lexical diversity. Then calcualed the length of unique words to derive the vocabulary size and highest number of vocabulary to calculate the vocabulary score. Similarly, calculated # of unique words and maximum number of word counts to derive long word score. Following to that added Lexical Diversity, Vocab. Score and Long Word Score to calculate the difficilty of text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_diff: 1.0, v_score: 0.5, lw_score: 0.3, lex_diver: 0.1, url: http://www.gutenberg.org/files/19533/19533.txt\n",
      "text_diff: 1.5, v_score: 0.7, lw_score: 0.7, lex_diver: 0.2, url: http://www.gutenberg.org/files/24681/24681.txt\n",
      "text_diff: 1.1, v_score: 0.4, lw_score: 0.4, lex_diver: 0.2, url: http://www.gutenberg.org/files/33912/33912.txt\n",
      "text_diff: 0.6, v_score: 0.0, lw_score: 0.1, lex_diver: 0.5, url: http://www.gutenberg.org/files/18448/18448.txt\n",
      "text_diff: 1.0, v_score: 0.4, lw_score: 0.4, lex_diver: 0.2, url: http://www.gutenberg.org/files/21281/21281.txt\n",
      "text_diff: 2.1, v_score: 1.0, lw_score: 1.0, lex_diver: 0.1, url: http://www.gutenberg.org/files/23319/23319.txt\n",
      "text_diff: 1.0, v_score: 0.4, lw_score: 0.3, lex_diver: 0.2, url: http://www.gutenberg.org/files/29269/29269.txt\n",
      "text_diff: 1.4, v_score: 0.6, lw_score: 0.7, lex_diver: 0.1, url: http://www.gutenberg.org/files/27632/27632.txt\n",
      "text_diff: 1.3, v_score: 0.7, lw_score: 0.5, lex_diver: 0.1, url: http://www.gutenberg.org/files/25269/25269.txt\n",
      "text_diff: 1.4, v_score: 0.6, lw_score: 0.6, lex_diver: 0.2, url: http://www.gutenberg.org/files/32282/32282.txt\n"
     ]
    }
   ],
   "source": [
    "def score_vocabulary_size():\n",
    "    highest_vocab = 0\n",
    "    highest_long_word_count = 0\n",
    "    \n",
    "    # calculate vocabulary size \n",
    "    # calculate highest vocabulary size \n",
    "    for text in all_texts:\n",
    "        corpus = text['corpus']\n",
    "        words = get_words_from_corpus(corpus)\n",
    "        unique_words = get_unique_words(words)\n",
    "\n",
    "        vocabulary_size = len(unique_words)\n",
    "        long_word_count = get_long_word_count(unique_words)\n",
    "        lexical_diversity = get_lexical_diversity(words, unique_words)\n",
    "\n",
    "        text['vocabulary_size'] = vocabulary_size\n",
    "        text['long_word_count'] = long_word_count\n",
    "        text['lexical_diversity'] = lexical_diversity\n",
    "\n",
    "        if vocabulary_size > highest_vocab:\n",
    "            highest_vocab = vocabulary_size\n",
    "        if long_word_count > highest_long_word_count:\n",
    "            highest_long_word_count = long_word_count\n",
    "\n",
    "    # score each item\n",
    "    for text in all_texts:\n",
    "        vocabulary_size = text['vocabulary_size']\n",
    "        long_word_count = text['long_word_count']\n",
    "        lexical_diversity = text['lexical_diversity']\n",
    "\n",
    "        vocabulary_score = get_vocabulary_score(vocabulary_size, highest_vocab)\n",
    "        long_word_score = get_vocabulary_score(long_word_count, highest_long_word_count)\n",
    "        text_difficulty = vocabulary_score + long_word_score + lexical_diversity\n",
    "\n",
    "        text['vocabulary_score'] = vocabulary_score\n",
    "        text['long_word_score'] = long_word_score\n",
    "        print('text_diff: {0:.1f}, v_score: {1:.1f}, lw_score: {2:.1f}, lex_diver: {3:.1f}, url: {4}'.format(text_difficulty, vocabulary_score, long_word_score, lexical_diversity, text['url']))\n",
    "\n",
    "score_vocabulary_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary ## \n",
    "\n",
    "**Used technology book shelf and it appears that**\n",
    "\n",
    "## Mechanical Drawing Self-Taught , by Joshua Rose ##\n",
    "\n",
    "**has highest text difficulty.**\n",
    "\n",
    "**while comparing to Homework 1 grade books LD, it appears that the technology/engineering book appears to have higher lexical diversity which theoratically make sense**\n",
    "\n",
    "**Example LD from Homework 1**\n",
    "\n",
    "*text_diversity 0.0013633158348350701 | level:first*\n",
    "\n",
    "*text_diversity 0.0005173951828724353 | level:third*\n",
    "\n",
    "*text_diversity 0.00014538990101921624 | level:fifth*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
